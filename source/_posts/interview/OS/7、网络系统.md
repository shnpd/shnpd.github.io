---
title: 7、网络系统
toc: true
date: 2025-03-03 00:00:00
tags: 操作系统
categories: 
	- 知识点整理
	- 操作系统

---

点击阅读更多查看文章内容<!--more-->

## 什么是零拷贝？

磁盘可以说是计算机系统最慢的硬件之一，读写速度相差内存 10 倍以上，所以针对优化磁盘的技术非常的多，比如零拷贝、直接 I/O、异步 I/O 等等，这些优化的目的就是为了提高系统的吞吐量，另外操作系统内核中的磁盘高速缓存区，可以有效的减少磁盘的访问次数。

### 为什么要有DMA技术？

在没有 DMA 技术前，I/O 的过程是这样的： 

- CPU 发出对应的指令给磁盘控制器，然后返回； 
- 磁盘控制器收到指令后，于是就开始准备数据，会把数据放入到磁盘控制器的内部缓冲区中，然后产生一个中断； 
- CPU 收到中断信号后，停下手头的工作，接着把磁盘控制器的缓冲区的数据一次一个字节地读进自己的寄存器，然后再把寄存器里的数据写入到内存，而在数据传输的期间 CPU 是无法执行其他任务的。

<img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost2/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E9%9B%B6%E6%8B%B7%E8%B4%9D/I_O%20%E4%B8%AD%E6%96%AD.png" alt="img" style="zoom:50%;" />

可以看到，整个数据的传输过程，都要需要 CPU 亲自参与搬运数据的过程，而且这个过程，CPU 是不能做其他事情的。 

 DMA 技术，也就是直接内存访问（Direct Memory Access） 技术，在进行 I/O 设备和内存的数据传输的时候，数据搬运的工作全部交给 **DMA 控制器**，而 CPU 不再参与任何与数据搬运相关的事情，这样 CPU 就可以去处理别的事务。

<img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost2/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E9%9B%B6%E6%8B%B7%E8%B4%9D/DRM%20I_O%20%E8%BF%87%E7%A8%8B.png" alt="img" style="zoom:50%;" />

### 传统文件传输的问题

如果服务端要提供文件传输的功能，我们能想到的最简单的方式是：将磁盘上的文件读取出来，然后通过网络协议发送给客户端。

传统 I/O 的工作方式是，数据读取和写入是从用户空间到内核空间来回复制，而内核空间的数据是通过操作系统层面的 I/O 接口从磁盘读取或写入。

<img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost2/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E9%9B%B6%E6%8B%B7%E8%B4%9D/%E4%BC%A0%E7%BB%9F%E6%96%87%E4%BB%B6%E4%BC%A0%E8%BE%93.png" alt="img" style="zoom:50%;" />

首先，期间共发生了 **4 次用户态与内核态的上下文切换**，因为发生了两次系统调用，一次是 read() ，一次是 write()，每次系统调用都得先从用户态切换到内核态，等内核完成任务后，再从内核态切换回用户态。 

上下文切换到成本并不小，一次切换需要耗时几十纳秒到几微秒，虽然时间看上去很短，但是在高并发的场景下，这类时间容易被累积和放大，从而影响系统的性能。 

其次，还发生了 **4 次数据拷贝**，其中两次是 DMA 的拷贝，另外两次则是通过 CPU 拷贝的，下面说一下这个过程： 

- 第一次拷贝，把磁盘上的数据拷贝到操作系统内核的缓冲区里，这个拷贝的过程是通过 DMA 搬运的。 
- 第二次拷贝，把内核缓冲区的数据拷贝到用户的缓冲区里，于是我们应用程序就可以使用这部分数据了，这个拷贝到过程是由 CPU 完成的。 
- 第三次拷贝，把刚才拷贝到用户的缓冲区里的数据，再拷贝到内核的 socket 的缓冲区里，这个过程依然还是由 CPU 搬运的。 
- 第四次拷贝，把内核的 socket 缓冲区里的数据，拷贝到网卡的缓冲区里，这个过程又是由 DMA 搬运的

所以，要想提高文件传输的性能，就需要减少「用户态与内核态的上下文切换」和「内存拷贝」的次数。

> 先来看看，如何减少「用户态与内核态的上下文切换」的次数呢？ 

读取磁盘数据的时候，之所以要发生上下文切换，这是因为用户空间没有权限操作磁盘或网卡，内核的权限最高，这些操作设备的过程都需要交由操作系统内核来完成，所以一般要通过内核去完成某些任务的时候，就需要使用操作系统提供的系统调用函数。 而一次系统调用必然会发生 2 次上下文切换：首先从用户态切换到内核态，当内核执行完任务后，再切换回用户态交由进程代码执行。 所以，要想减少上下文切换到次数，就要**减少系统调用的次数**。 

> 再来看看，如何减少「数据拷贝」的次数？ 

在前面我们知道了，传统的文件传输方式会历经 4 次数据拷贝，而且这里面，「从内核的读缓冲区拷贝到用户的缓冲区里，再从用户的缓冲区里拷贝到 socket 的缓冲区里」，这个过程是没有必要的。 因为文件传输的应用场景中，在用户空间我们并不会对数据「再加工」，所以数据实际上可以不用搬运到用户空间，因此**用户的缓冲区是没有必要存在的**。

---

### 如何实现零拷贝

零拷贝技术实现的方式通常有 2 种： 

- mmap + write 
- sendfile 

下面就谈一谈，它们是如何减少「上下文切换」和「数据拷贝」的次数。

**mmap + write**

在前面我们知道，read() 系统调用的过程中会把内核缓冲区的数据拷贝到用户的缓冲区里，于是为了减少这一步开销，我们可以用 mmap() 替换 read() 系统调用函数。 
buf = mmap(file, len); write(sockfd, buf, len); 
mmap() 系统调用函数会直接把内核缓冲区里的数据「映射」到用户空间，这样，操作系统内核与用户空间就不需要再进行任何的数据拷贝操作。

<img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost2/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E9%9B%B6%E6%8B%B7%E8%B4%9D/mmap%20%2B%20write%20%E9%9B%B6%E6%8B%B7%E8%B4%9D.png" alt="img" style="zoom:50%;" />

具体过程如下：

- 应用进程调用了 mmap() 后，DMA 会把磁盘的数据拷贝到内核的缓冲区里。接着，应用进程跟操作系统内核「共享」这个缓冲区； 
- 应用进程再调用 write()，操作系统直接将内核缓冲区的数据拷贝到 socket 缓冲区中，这一切都发生在内核态，由 CPU 来搬运数据； 
- 最后，把内核的 socket 缓冲区里的数据，拷贝到网卡的缓冲区里，这个过程是由 DMA 搬运的。 

我们可以得知，通过使用 mmap() 来代替 read()， 可以**减少一次数据拷贝**的过程。 但这还不是最理想的零拷贝，因为仍然需要通过 CPU 把内核缓冲区的数据拷贝到 socket 缓冲区里，而且仍然需要 4 次上下文切换，因为系统调用还是 2 次。

**sendfile**

在 Linux 内核版本 2.1 中，提供了一个专门发送文件的系统调用函数 sendfile()，函数形式如下：

```c
#include <sys/socket.h>
ssize_t sendfile(int out_fd, int in_fd, off_t *offset, size_t count);
```

它可以替代前面的 read() 和 write() 这两个系统调用，这样就可以减少一次系统调用，也就减少了 2 次上下文切换的开销。 其次，该系统调用，可以**直接把内核缓冲区里的数据拷贝到 socket 缓冲区**里，不再拷贝到用户态，这样就只有 2 次上下文切换，和 3 次数据拷贝。如下图：

<img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost2/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E9%9B%B6%E6%8B%B7%E8%B4%9D/senfile-3%E6%AC%A1%E6%8B%B7%E8%B4%9D.png" alt="img" style="zoom:50%;" />

但是这还不是真正的零拷贝技术，如果网卡支持 SG-DMA（The Scatter-Gather Direct Memory Access）技术（和普通的 DMA 有所不同），我们可以进一步减少通过 CPU 把内核缓冲区里的数据拷贝到 socket 缓冲区的过程。

从 Linux 内核 2.4 版本开始起，对于支持网卡支持 SG-DMA 技术的情况下， sendfile() 系统调用的过程发生了点变化，具体过程如下：

-  第一步，通过 DMA 将磁盘上的数据拷贝到内核缓冲区里； 
-  第二步，缓冲区描述符和数据长度传到 socket 缓冲区，这样网卡的 SG-DMA 控制器就可以直接将内核缓存中的数据拷贝到网卡的缓冲区里，此过程不需要将数据从操作系统内核缓冲区拷贝到 socket 缓冲区中，这样就减少了一次数据拷贝； 

所以，这个过程之中，只进行了 2 次数据拷贝，如下图：

<img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost2/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E9%9B%B6%E6%8B%B7%E8%B4%9D/senfile-%E9%9B%B6%E6%8B%B7%E8%B4%9D.png" alt="img" style="zoom:50%;" />

这就是所谓的零拷贝（Zero-copy）技术，因为我们没有在内存层面去拷贝数据，也就是说全程没有通过 CPU 来搬运数据，所有的数据都是通过 DMA 来进行传输的。

零拷贝技术的文件传输方式相比传统文件传输的方式，减少了 2 次上下文切换和数据拷贝次数，只需要 2 次上下文切换和数据拷贝次数，就可以完成文件的传输，而且 2 次的数据拷贝过程，都不需要通过 CPU，2 次都是由 DMA 来搬运。 所以，总体来看，零拷贝技术可以把文件传输的性能提高至少一倍以上。

**使用零拷贝技术的项目**

kafka、nginx

---

## 线程池

线程是运行在进程中的一个“逻辑流”，单进程中可以运行多个线程，同进程里的线程可以共享进程的部分资源，比如文件描述符列表、进程空间、代码、全局数据、堆、共享库等，这些共享些资源在上下文切换时不需要切换，而只需要切换线程的私有数据、寄存器等不共享的数据，因此同一个进程下的线程上下文切换的开销要比进程小得多。 

当服务器与客户端 TCP 完成连接后，通过 pthread_create() 函数创建线程，然后将「已连接 Socket」的文件描述符传递给线程函数，接着在线程里和客户端进行通信，从而达到并发处理的目的。 

如果每来一个连接就创建一个线程，线程运行完后，还得操作系统还得销毁线程，虽说线程切换的上写文开销不大，但是如果频繁创建和销毁线程，系统开销也是不小的。 

那么，我们可以使用线程池的方式来避免线程的频繁创建和销毁，所谓的线程池，就是提前创建若干个线程，这样当由新连接建立时，将这个已连接的 Socket 放入到一个队列里，然后线程池里的线程负责从队列中取出「已连接 Socket 」进行处理。

<img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E5%A4%9A%E8%B7%AF%E5%A4%8D%E7%94%A8/%E7%BA%BF%E7%A8%8B%E6%B1%A0.png" alt="img" style="zoom: 50%;" />

需要注意的是，这个队列是全局的，每个线程都会操作，为了避免多线程竞争，线程在操作这个队列前要加锁。 上面基于进程或者线程模型的，其实还是有问题的。

新到来一个 TCP 连接，就需要分配一个进程或者线程，那么如果要达到 C10K （C 是 Client 单词首字母缩写，C10K 就是单机同时处理 1 万个请求的问题。），意味着要一台机器维护 1 万个连接，相当于要维护 1 万个进程/线程，操作系统就算死扛也是扛不住的。

---

## I/O多路复用

既然为每个请求分配一个进程/线程的方式不合适，那有没有可能只使用一个进程来维护多个 Socket 呢？答案是有的，那就是 I/O 多路复用技术。

<img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E5%A4%9A%E8%B7%AF%E5%A4%8D%E7%94%A8/%E5%A4%9A%E8%B7%AF%E5%A4%8D%E7%94%A8.png" alt="img" style="zoom:50%;" />

一个进程虽然任一时刻只能处理一个请求，但是处理每个请求的事件时，耗时控制在 1 毫秒以内，这样 1 秒内就可以处理上千个请求，把时间拉长来看，多个请求复用了一个进程，这就是多路复用，这种思想很类似一个 CPU 并发多个进程，所以也叫做时分多路复用。 

 `elect`、`poll` 和 `epoll` 是 Linux 系统中用于 I/O 多路复用的机制，它们**允许一个进程同时监控多个文件描述符（如套接字、管道等）**，并在这些文件描述符中的任何一个变得可读、可写或出现异常时通知进程。这些机制在网络编程中非常有用，尤其是在需要处理大量并发连接时。

select/poll/epoll 是如何获取网络事件的呢？在获取事件时，先把所有连接（文件描述符）传给内核，再由内核返回产生了事件的连接，然后在用户态中再处理这些连接对应的请求即可。 select/poll/epoll 这是三个多路复用接口，都能实现 C10K 吗？接下来，我们分别说说它们。

### select

select 实现多路复用的方式是，将已连接的 Socket 都放到一个**文件描述符集合**，然后调用 select 函数将文件描述符集合**拷贝**到内核里，让内核来检查是否有网络事件产生，检查的方式很粗暴，就是通过**遍历文件描述符**集合的方式，当检查到有事件产生后，将此 Socket 标记为可读或可写， 接着再把整个文件描述符集合**拷贝**回用户态里，然后用户态还需要再通过**遍历**的方法找到可读或可写的 Socket，然后再对其处理。 

所以，对于 select 这种方式，需要进行 2 次「遍历」文件描述符集合，一次是在内核态里，一个次是在用户态里 ，而且还会发生 2 次「拷贝」文件描述符集合，先从用户空间传入内核空间，由内核修改后，再传出到用户空间中。 

### poll

poll 与 `select` 不同，`poll` 使用一个数组来存储文件描述符，因此没有文件描述符数量的限制（仅受系统资源限制）。每次调用 `poll` 时，仍然需要将文件描述符集合从用户空间拷贝到内核空间。内核仍然需要遍历所有文件描述符来检查状态。

`poll` 的工作流程

1. **初始化**：将需要监控的文件描述符列表传递给 `poll()`。
2. 调用 `poll()`：
   - 内核遍历所有文件描述符，检查是否有事件（如可读、可写）就绪。
   - 返回就绪的文件描述符数量，并标记哪些文件描述符就绪。
3. **应用程序遍历所有文件描述符**，找到就绪的进行处理。
4. **重复步骤 1~3**，每次调用 `poll()` 都需要重新传递所有文件描述符。

缺点：

- 文件描述符数量多时，遍历开销大（时间复杂度 O(n)）。
- 每次调用需要全量传递文件描述符列表，内存拷贝开销大

### epoll

`epoll` 是 Linux 2.6 引入的 I/O 多路复用机制，旨在解决 `select` 和 `poll` 的性能问题。

- `epoll` 使用一个**红黑树**来存储需要监控的文件描述符，因此添加、删除和查找文件描述符的效率很高。
- epoll 使用事件驱动的机制，内核里维护了一个链表来记录**就绪事件**，当某个 socket 有事件发生时，通过回调函数内核会将其加入到这个就绪事件列表中，当用户调用 epoll_wait() 函数时，只会返回有事件发生的文件描述符的个数，不需要像select/poll 那样轮询扫描整个 socket 集合，大大提高了检测的效率。

<img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E5%A4%9A%E8%B7%AF%E5%A4%8D%E7%94%A8/epoll.png" alt="img" style="zoom:50%;" />

`epoll` 的工作流程

1. 初始化：
   - 调用 `epoll_create()` 创建一个 epoll 实例。
   - 调用 `epoll_ctl()` 向 epoll 实例注册需要监控的文件描述符和事件。
2. 调用 `epoll_wait()`：
   - 内核直接返回已就绪的文件描述符列表（无需遍历所有文件描述符）。
3. **应用程序遍历就绪列表**，处理就绪的文件描述符。
4. **重复步骤 2~3**，无需重新注册文件描述符。

优点：

- 时间复杂度为 O(1)，仅处理就绪的文件描述符。
- 内核通过红黑树管理文件描述符，高效查找和更新。

**触发模式**

- **水平触发（LT, Level-Triggered）**：
  - 当被监控的 Socket 上有可读事件发生时，服务器端不断地从 epoll_wait 中苏醒，直到内核缓冲区数据被 read 函数读完才结束，**目的是告诉我们有数据需要读取**；
  - **适用场景**：适合编程简单的场景，确保数据能完全处理。
  - **示例**：`poll` 仅支持 LT 模式，`epoll` 默认也是 LT 模式。
- **边缘触发（ET, Edge-Triggered）**：
  - 当被监控的 Socket 描述符上**有可读事件发生时，服务器端只会从 epoll_wait 中苏醒一次**，即使进程没有调用 read 函数从内核读取数据，也依然只苏醒一次，因此我们程序要保证**一次性将内核缓冲区的数据读取完**；
  - **适用场景**：需要高性能的场景，但应用程序必须一次性处理完所有数据，否则可能丢失事件。
  - **示例**：`epoll` 支持 ET 模式。

- **水平触发（LT）的行为**

  - **核心特点**：
    - 只要 socket 的某个状态（如可读、可写）**持续存在**，每次调用 `epoll_wait` 都会报告该事件，直到状态被处理完毕。
    - 事件通知是“宽容”的，允许应用程序分多次处理一个状态下的多个事件。

  - **示例场景**：
    - **场景**：socket 接收缓冲区中有 2KB 数据未读取。
    - **第一次调用 `epoll_wait`**：报告该 socket 可读。
    - **处理**：应用程序读取 1KB 数据，还剩 1KB。
    - **第二次调用 `epoll_wait`**：仍然会报告该 socket 可读，因为缓冲区中还有数据。
    - **处理**：应用程序继续读取剩余的 1KB，缓冲区清空。
    - **第三次调用 `epoll_wait`**：不再报告该 socket 可读。

  - **总结**：
    - LT 模式下，未处理完的事件会持续通知，适合需要分批次处理数据的场景。

- **边缘触发（ET）的行为**

  - **核心特点**：
    - 仅在 socket 状态**从无到有发生变化时**通知一次。
    - 如果应用程序未完全处理所有事件（例如未读完所有数据），**剩余事件不会触发新的通知**，即使状态仍然存在。

  - **示例场景**：
    - **场景**：socket 接收缓冲区中有 2KB 数据未读取。
    - **第一次调用 `epoll_wait`**：报告该 socket 可读（状态从“不可读”变为“可读”）。
    - **处理**：应用程序读取 1KB 数据，还剩 1KB。
    - **第二次调用 `epoll_wait`**：**不会报告该 socket 可读**，因为状态未发生新的变化（缓冲区仍有数据，但未再次从空变为非空）。
    - **剩余数据**：剩下的 1KB 数据会一直滞留在缓冲区中，直到有新数据到达（再次触发状态变化）。

  - **关键问题**：
    - **必须一次性处理所有事件**：在 ET 模式下，应用程序必须循环读取数据，直到返回 `EAGAIN` 或 `EWOULDBLOCK`，确保缓冲区被完全清空。

### 总结

最基础的 TCP 的 Socket 编程，它是阻塞 I/O 模型，基本上只能一对一通信，那为了服务更多的客户端，我们需要改进网络 I/O 模型。 

比较传统的方式是使用多进程/线程模型，每来一个客户端连接，就分配一个进程/线程，然后后续的读写都在对应的进程/线程，这种方式处理 100 个客户端没问题，但是当客户端增大到 10000 个时，10000 个进程/线程的调度、上下文切换以及它们占用的内存，都会成为瓶颈。 

为了解决上面这个问题，就出现了 I/O 的多路复用，可以只在一个进程里处理多个文件的 I/O，Linux 下有三种提供 I/O 多路复用的 API，分别是：select、poll、epoll。

select 和 poll 并没有本质区别，它们内部都是使用「线性结构」来存储进程关注的 Socket 集合。 在使用的时候，首先需要把关注的 Socket 集合通过 select/poll 系统调用从用户态拷贝到内核态，然后由内核检测事件，当有网络事件产生时，内核需要遍历进程关注 Socket 集合，找到对应的 Socket，并设置其状态为可读/可写，然后把整个 Socket 集合从内核态拷贝到用户态，用户态还要继续遍历整个 Socket 集合找到可读/可写的 Socket，然后对其处理。 很明显发现，select 和 poll 的缺陷在于，当客户端越多，也就是 Socket 集合越大，Socket 集合的遍历和拷贝会带来很大的开销，因此也很难应对 C10K。 

epoll 是解决 C10K 问题的利器，通过两个方面解决了 select/poll 的问题。 epoll 在内核里使用「红黑树」来关注进程所有待检测的 Socket，红黑树是个高效的数据结构，增删改一般时间复杂度是 O(logn)，通过对这棵黑红树的管理，不需要像 select/poll 在每次操作时都传入整个 Socket 集合，减少了内核和用户空间大量的数据拷贝和内存分配。 epoll 使用**事件驱动**的机制，内核里维护了一个「链表」来记录就绪事件，只将有事件发生的 Socket 集合传递给应用程序，不需要像 select/poll 那样轮询扫描整个集合（包含有和无事件的 Socket ），大大提高了检测的效率。 

而且，epoll 支持边缘触发和水平触发的方式，而 select/poll 只支持水平触发，一般而言，边缘触发的方式会比水平触发的效率高。

---

## 一致性哈希

### 如何分配请求？

引入一个中间的负载均衡层，让它将外界的请求「轮流」的转发给内部的集群。比如集群有 3 个节点，外界请求有 3 个，那么每个节点都会处理 1 个请求，达到了分配请求的目的

![img](https://cdn.xiaolincoding.com//mysql/other/d3279ad754257977f98e702cb156e9cf.png)

考虑到每个节点的硬件配置有所区别，我们可以引入权重值，将硬件配置更好的节点的权重值设高，然后根据各个节点的权重值，按照一定比重分配在不同的节点上，让硬件配置更好的节点承担更多的请求，这种算法叫做加权轮询。 

加权轮询算法使用场景是建立在**每个节点存储的数据都是相同的**前提。所以，每次读数据的请求，访问任意一个节点都能得到结果。 

但是，加权轮询算法是无法应对「分布式系统（数据分片的系统）」的，因为分布式系统中，**每个节点存储的数据是不同的**。 当我们想提高系统的容量，就会将数据水平切分到不同的节点来存储，也就是将数据分布到了不同的节点。

比如**一个分布式 KV（key-valu） 缓存系统，某个 key 应该到哪个或者哪些节点上获得，应该是确定的**，不是说任意访问一个节点都可以得到缓存结果的。 因此，我们要想一个能应对分布式系统的负载均衡算法

---

### 普通哈希的问题

有的同学可能很快就想到了：哈希算法。因为对同一个关键字进行哈希计算，每次计算都是相同的值，这样就可以将某个 key 确定到一个节点了，可以满足分布式系统的负载均衡需求。 

哈希算法最简单的做法就是进行取模运算，比如分布式系统中有 3 个节点，基于 hash(key) % 3 公式对数据进行了映射。 如果客户端要获取指定 key 的数据，通过下面的公式可以定位节点：hash(key) % 3

如果经过上面这个公式计算后得到的值是 0，就说明该 key 需要去第一个节点获取。 但是有一个很致命的问题，如果**节点数量发生了变化**，也就是在对系统做扩容或者缩容后，取模的值发生了改变，数据对应的服务器也发生了改变。必须迁移改变了映射关系的数据，否则会出现查询不到数据的问题。 

假设总数据条数为 M，哈希算法在面对节点数量变化时，最坏情况下所有数据都需要迁移，所以它的数据迁移规模是 O(M)，这样数据的迁移成本太高了。 所以，我们应该要重新想一个新的算法，来避免分布式系统在扩容或者缩容时，发生过多的数据迁移。

---

### 一致性哈希

一致哈希算法也用了取模运算，但与哈希算法不同的是，哈希算法是对节点的数量进行取模运算，而一致哈希算法是对 2^32 进行取模运算，是一个固定的值。 

我们可以把一致哈希算法是对 2^32 进行取模运算的结果值组织成一个圆环，就像钟表一样，钟表的圆可以理解成由 60 个点组成的圆，而此处我们把这个圆想象成由 2^32 个点组成的圆，这个圆环被称为哈希环，如下图：

<img src="https://cdn.xiaolincoding.com//mysql/other/0ea3960fef48d4cbaeb4bec4345301e7.png" alt="img" style="zoom:67%;" />

一致性哈希要进行两步哈希： 

- 第一步：对存储节点进行哈希计算，也就是对存储节点做哈希映射，比如根据节点的 IP 地址进行哈希； 
- 第二步：当对数据进行存储或访问时，对数据进行哈希映射； 

所以，一致性哈希是指将「存储节点」和「数据」都映射到一个首尾相连的哈希环上。 问题来了，对「数据」进行哈希映射得到一个结果要怎么找到存储该数据的节点呢？ 答案是，**映射的结果值往顺时针的方向的找到第一个节点，就是存储该数据的节点**。 举个例子，有 3 个节点经过哈希计算，映射到了如下图的位置：

<img src="https://cdn.xiaolincoding.com//mysql/other/83d7f363643353c92d252e34f1d4f687.png" alt="img" style="zoom:50%;" />

接着，对要查询的 key-01 进行哈希计算，确定此 key-01 映射在哈希环的位置，然后从这个位置往顺时针的方向找到第一个节点，就是存储该 key-01 数据的节点。 比如，下图中的 key-01 映射的位置，往顺时针的方向找到第一个节点就是节点 A。

<img src="https://cdn.xiaolincoding.com//mysql/other/30c2c70721c12f9c140358fbdc5f2282.png" alt="img" style="zoom:50%;" />

所以，当需要对指定 key 的值进行读写的时候，要通过下面 2 步进行寻址： 

- 首先，对 key 进行哈希计算，确定此 key 在环上的位置； 
- 然后，从这个位置沿着顺时针方向走，遇到的第一节点就是存储 key 的节点。 

知道了一致哈希寻址的方式，我们来看看，如果增加一个节点或者减少一个节点会发生大量的数据迁移吗？ 假设节点数量从 3 增加到了 4，新的节点 D 经过哈希计算后映射到了下图中的位置：

<img src="https://cdn.xiaolincoding.com//mysql/other/f8909edef2f3949f8945bb99380baab3.png" alt="img" style="zoom:50%;" />

你可以看到，key-01、key-03 都不受影响，只有 key-02 需要被迁移节点 D。 假设节点数量从 3 减少到了 2，比如将节点 A 移除

<img src="https://cdn.xiaolincoding.com//mysql/other/31485046f1303b57d8aaeaab103ea7ab.png" alt="img" style="zoom:50%;" />

你可以看到，key-02 和 key-03 不会受到影响，只有 key-01 需要被迁移节点 B。 因此，在一致哈希算法中，如果增加或者移除一个节点，仅影响该节点在哈希环上顺时针相邻的后继节点，其它数据也不会受到影响。 

上面这些图中 3 个节点映射在哈希环还是比较分散的，所以看起来请求都会「均衡」到每个节点。 但是一致性哈希算法并不保证节点能够在哈希环上分布均匀，这样就会带来一个问题，会有**大量的请求集中在一个节点**上。 比如，下图中 3 个节点的映射位置都在哈希环的右半边：

<img src="https://cdn.xiaolincoding.com//mysql/other/d528bae6fcec2357ba2eb8f324ad9fd5.png" alt="img" style="zoom:50%;" />

这时候有一半以上的数据的寻址都会找节点 A，也就是访问请求主要集中的节点 A 上， 另外，在这种节点分布不均匀的情况下，进行容灾与扩容时，哈希环上的相邻节点容易受到过大影响，容易发生雪崩式的连锁反应。 比如，上图中如果节点 A 被移除了，当节点 A 宕机后，根据一致性哈希算法的规则，其上数据应该全部迁移到相邻的节点 B 上，这样，节点 B 的数据量、访问量都会迅速增加很多倍，一旦新增的压力超过了节点 B 的处理能力上限，就会导致节点 B 崩溃，进而形成雪崩式的连锁反应。 所以，一致性哈希算法虽然减少了数据迁移量，但是存在节点分布不均匀的问题。

要想解决节点能在哈希环上分配不均匀的问题，就是要有大量的节点，节点数越多，哈希环上的节点分布的就越均匀。 但问题是，实际中我们没有那么多节点。所以这个时候我们就加入**虚拟节点**，也就是对一个真实节点做多个副本。 具体做法是，不再将真实节点映射到哈希环上，而是将虚拟节点映射到哈希环上，并将虚拟节点映射到实际节点，所以这里有「两层」映射关系。 

比如对每个节点分别设置 3 个虚拟节点： 

- 对节点 A 加上编号来作为虚拟节点：A-01、A-02、A-03 
- 对节点 B 加上编号来作为虚拟节点：B-01、B-02、B-03 
- 对节点 C 加上编号来作为虚拟节点：C-01、C-02、C-03 

引入虚拟节点后，原本哈希环上只有 3 个节点的情况，就会变成有 9 个虚拟节点映射到哈希环上，哈希环上的节点数量多了 3 倍。

<img src="https://cdn.xiaolincoding.com//mysql/other/dbb57b8d6071d011d05eeadd93269e13.png" alt="img" style="zoom:50%;" />

你可以看到，节点数量多了后，节点在哈希环上的分布就相对均匀了。这时候，如果有访问请求寻址到「A-01」这个虚拟节点，接着再通过「A-01」虚拟节点找到真实节点 A，这样请求就能访问到真实节点 A 了。 

 另外，虚拟节点除了会提高节点的均衡度，还会提高系统的稳定性。当节点变化时，会有不同的节点共同分担系统的变化，因此稳定性更高。 比如，当某个节点被移除时，对应该节点的多个虚拟节点均会移除，而这些虚拟节点按顺时针方向的下一个虚拟节点，可能会对应不同的真实节点，即这些不同的真实节点共同分担了节点变化导致的压力。

 而且，有了虚拟节点后，还可以为硬件配置更好的节点增加权重，比如对权重更高的节点增加更多的虚拟机节点即可。 因此，带虚拟节点的一致性哈希方法不仅适合硬件配置不同的节点的场景，而且适合节点规模会发生变化的场景。