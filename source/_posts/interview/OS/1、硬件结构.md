---
title: 1、硬件结构
toc: true
date: 2025-03-03 00:00:00
tags: 操作系统
categories: 
	- 知识点整理
	- 操作系统


---

点击阅读更多查看文章内容<!--more-->

## CPU是如何执行程序的？

### 冯诺依曼模型

冯诺依曼模型：运算器、控制器、存储器、输入设备、输出设备

运算器、控制器是在中央处理器里的，存储器就是我们常见的内存，输入输出设备则是计算机外接的设备。

存储单元和输入/输出设备要与中央处理器打交道就需要总线。

<img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost2/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E7%A8%8B%E5%BA%8F%E6%89%A7%E8%A1%8C/%E5%86%AF%E8%AF%BA%E4%BE%9D%E6%9B%BC%E6%A8%A1%E5%9E%8B.png" alt="img" style="zoom: 67%;" />

**内存**

 我们的程序和数据都是存储在内存，存储的区域是线性的。 在计算机数据存储中，存储数据的基本单位是字节（byte），1 字节等于 8 位（8 bit）。每一个字节都对应一个内存地址。 内存的地址是从 0 开始编号的，然后自增排列，最后一个地址为内存总字节数 - 1，这种结构好似我们程序里的数组，所以内存的读写任何一个数据的速度都是一样的。

### 中央处理器

中央处理器也就是我们常说的 CPU，32 位和 64 位 CPU 最主要区别在于一次能计算多少字节数据： 32 位 CPU 一次可以计算 4 个字节； 64 位 CPU 一次可以计算 8 个字节； 这里的 32 位和 64 位，通常称为 CPU 的位宽，代表的是 CPU 一次可以计算（运算）的数据量。 之所以 CPU 要这样设计，是为了能计算更大的数值，如果是 8 位的 CPU，那么一次只能计算 1 个字节 0~255 范围内的数值，这样就无法一次完成计算 10000 * 500 ，于是为了能一次计算大数的运算，CPU 需要支持多个 byte 一起计算，所以 CPU 位宽越大，可以计算的数值就越大，比如说 32 位 CPU 能计算的最大整数是 4294967295。 

CPU 内部还有一些组件，常见的有寄存器、控制单元和逻辑运算单元等。其中，控制单元负责控制 CPU 工作，逻辑运算单元负责计算，而寄存器可以分为多种类，每种寄存器的功能又不尽相同。 CPU 中的寄存器主要作用是存储计算时的数据，你可能好奇为什么有了内存还需要寄存器？原因很简单，因为内存离 CPU 太远了，而寄存器就在 CPU 里，还紧挨着控制单元和逻辑运算单元，自然计算时速度会很快

常见的寄存器种类： 

- 通用寄存器，用来存放需要进行运算的数据，比如需要进行加和运算的两个数据。 
- 程序计数器，用来存储 CPU 要执行下一条指令「所在的内存地址」，注意不是存储了下一条要执行的指令，此时指令还在内存中，程序计数器只是存储了下一条指令「的地址」。
- 指令寄存器，用来存放当前正在执行的指令，也就是指令本身，指令被执行完成之前，指令都存储在这里

### 总线

总线是用于 CPU 和内存以及其他设备之间的通信，总线可分为 3 种： 

- 地址总线，用于指定 CPU 将要操作的内存地址； 
- 数据总线，用于读写内存的数据； 
- 控制总线，用于发送和接收信号，比如中断、设备复位等信号，CPU 收到信号后自然进行响应，这时也需要控制总线； 

当 CPU 要读写内存数据的时候，一般需要通过下面这三个总线： 

- 首先要通过「地址总线」来指定内存的地址； 
- 然后通过「控制总线」控制是读或写命令； 
- 最后通过「数据总线」来传输数据；

### 输入、输出设备

输入设备向计算机输入数据，计算机经过计算后，把数据输出给输出设备。期间，如果输入设备是键盘，按下按键时是需要和 CPU 进行交互的，这时就需要用到控制总线了。

### 程序执行的基本过程

- 第一步，CPU 读取「程序计数器」的值，这个值是指令的内存地址，然后 CPU 的「控制单元」操作「地址总线」指定需要访问的内存地址，接着通知内存设备准备数据，数据准备好后通过「数据总线」将指令数据传给 CPU，CPU 收到内存传来的数据后，将这个指令数据存入到「指令寄存器」。 
- 第二步，「程序计数器」的值自增，表示指向下一条指令。这个自增的大小，由 CPU 的位宽决定，比如 32 位的 CPU，指令是 4 个字节，需要 4 个内存地址存放，因此「程序计数器」的值会自增 4； 
- 第三步，CPU 分析「指令寄存器」中的指令，确定指令的类型和参数，如果是计算类型的指令，就把指令交给「逻辑运算单元」运算；如果是存储类型的指令，则交由「控制单元」执行； 

简单总结一下就是，一个程序执行的时候，CPU 会根据程序计数器里的内存地址，从内存里面把需要执行的指令读取到指令寄存器里面执行，然后根据指令长度自增，开始顺序读取下一条指令。 CPU 从程序计数器读取指令、到执行、再到下一条指令，这个过程会不断循环，直到程序执行结束，这个不断循环的过程被称为 CPU 的指令周期

## 存储器

存储器的级别：

<img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost2/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E5%AD%98%E5%82%A8%E7%BB%93%E6%9E%84/%E5%AD%98%E5%82%A8%E5%8C%BA%E5%88%86%E7%BA%A7.png" alt="img" style="zoom:67%;" />

---

## 如何写出让CPU跑的更快的代码

提高cpu缓存的命中率：

假设要遍历二维数组，有以下两种形式，虽然代码执行结果是一样，但你觉得哪种形式效率最高呢？为什么高呢？ 

<img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost3@main/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/CPU%E7%BC%93%E5%AD%98/%E9%81%8D%E5%8E%86%E6%95%B0%E7%BB%84.png" alt="img" style="zoom:50%;" />

经过测试，形式一 array\[i][j] 执行时间比形式二 array\[j][i] 快好几倍。 之所以有这么大的差距，是因为二维数组 array 所占用的内存是连续的，比如长度 N 的值是 2 的话，那么内存中的数组元素的布局顺序是这样的： 

<img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost3@main/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/CPU%E7%BC%93%E5%AD%98/%E6%95%B0%E7%BB%84%E5%86%85%E5%AD%98%E5%B8%83%E5%B1%80%E9%A1%BA%E5%BA%8F.png" alt="img" style="zoom:50%;" />

形式一用 array\[i][j] 访问数组元素的顺序，正是和内存中数组元素存放的顺序一致。当 CPU 访问 array\[0][0] 时，由于该数据不在 Cache 中，于是会「顺序」把跟随其后的 3 个元素从内存中加载到 CPU Cache，这样当 CPU 访问后面的 3 个数组元素时，就能在 CPU Cache 中成功地找到数据，这意味着缓存命中率很高，缓存命中的数据不需要访问内存，这便大大提高了代码的性能

而如果用形式二的 array\[j][i] 来访问，则访问的顺序就是： 

<img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost3@main/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/CPU%E7%BC%93%E5%AD%98/%E5%BD%A2%E5%BC%8F%E4%BA%8C%E8%AE%BF%E9%97%AE%E9%A1%BA%E5%BA%8F.png" alt="img" style="zoom:50%;" />

你可以看到，访问的方式跳跃式的，而不是顺序的，那么如果 N 的数值很大，那么操作 array\[j][i] 时，是没办法把 array\[j+1][i] 也读入到 CPU Cache 中的，既然 array\[j+1][i] 没有读取到 CPU Cache，那么就需要从内存读取该数据元素了。很明显，这种不连续性、跳跃式访问数据元素的方式，可能不能充分利用到了 CPU Cache 的特性，从而代码的性能不高。 那访问 array\[0][0] 元素时，CPU 具体会一次从内存中加载多少元素到 CPU Cache 呢？这个问题，在前面我们也提到过，这跟 CPU Cache Line 有关，它表示 CPU Cache 一次性能加载数据的大小，可以在 Linux 里通过 coherency_line_size 配置查看 它的大小，通常是 64 个字节

也就是说，当 CPU 访问内存数据时，如果数据不在 CPU Cache 中，则会一次性会连续加载 64 字节大小的数据到 CPU Cache，那么当访问 array\[0][0] 时，由于该元素不足 64 字节，于是就会往后顺序读取 array\[0][0]~array\[0][15] 到 CPU Cache 中。顺序访问的 array\[i][j] 因为利用了这一特点，所以就会比跳跃式访问的 array\[j][i] 要快。 因此，遇到这种遍历数组的情况时，按照内存布局顺序访问，将可以有效的利用 CPU Cache 带来的好处，这样我们代码的性能就会得到很大的提升

---

## CPU缓存一致性

那么如果数据写入 Cache 之后，内存与 Cache 相对应的数据将会不同，这种情况下 Cache 和内存数据都不一致了，于是我们肯定是要把 Cache 中的数据同步到内存里的。 问题来了，那在什么时机才把 Cache 中的数据写回到内存呢？

为了应对这个问题，下面介绍两种针对写入数据的方法： 写直达（Write Through） 写回（Write Back） 

### 写直达 

保持内存与 Cache 一致性最简单的方式是，把数据同时写入内存和 Cache 中，这种方法称为写直达（Write Through）

<img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost3@main/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/CPU%E7%BC%93%E5%AD%98%E4%B8%80%E8%87%B4%E6%80%A7/%E5%86%99%E7%9B%B4%E8%BE%BE.png" alt="img" style="zoom:50%;" />

缺点：无论数据在不在 Cache 里面，每次写操作都会写回到内存，这样写操作将会花费大量的时间，无疑性能会受到很大的影响

### 写回

当发生写操作时，新的数据仅仅被写入 Cache Block 里，只有当修改过的 Cache Block「被替换」时才需要写到内存中，减少了数据写回内存的频率，这样便可以提高系统的性能。

<img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost3@main/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/CPU%E7%BC%93%E5%AD%98%E4%B8%80%E8%87%B4%E6%80%A7/%E5%86%99%E5%9B%9E1.png" alt="img" style="zoom: 67%;" />

- 如果当发生写操作时，数据已经在 CPU Cache 里的话，则把数据更新到 CPU Cache 里，同时标记 CPU Cache 里的这个 Cache Block 为脏（Dirty）的，这个脏的标记代表这个时候，我们 CPU Cache 里面的这个 Cache Block 的数据和内存是不一致的，这种情况是不用把数据写到内存里的； 
- 如果当发生写操作时，数据所对应的 Cache Block 里存放的是「别的内存地址的数据」的话，就要检查这个 Cache Block 里的数据有没有被标记为脏的： 
  - 如果是脏的话，我们就要把这个 Cache Block 里的数据写回到内存，然后再把当前要写入的数据，先从内存读入到 Cache Block 里（注意，这一步不是没用的，具体为什么要这一步，可以看这个「回答 (opens new window)」），然后再把当前要写入的数据写入到 Cache Block，最后也把它标记为脏的； 
  - 如果不是脏的话，把当前要写入的数据先从内存读入到 Cache Block 里，接着将数据写入到这个 Cache Block 里，然后再把这个 Cache Block 标记为脏的就好了。

### 多核缓存一致性问题

当今 CPU 都是多核的，每个核心都有各自独立的 L1/L2 Cache，只有 L3 Cache 是多个核心之间共享的。所以，我们要确保多核缓存是一致性的，否则会出现错误的结果。 要想实现缓存一致性，关键是要满足 2 点： 

- 第一点是写传播，也就是当某个 CPU 核心发生写入操作时，需要把该事件广播通知给其他核心； 
- 第二点是事务的串行化，这个很重要，只有保证了这个，才能保障我们的数据是真正一致的，我们的程序在各个不同的核心上运行的结果也是一致的； （加锁）

基于总线嗅探机制的 MESI 协议，就满足上面了这两点，因此它是保障缓存一致性的协议。 MESI 协议，是已修改、独占、共享、已失效这四个状态的英文缩写的组合。整个 MSI 状态的变更，则是根据来自本地 CPU 核心的请求，或者来自其他 CPU 核心通过总线传输过来的请求，从而构成一个流动的状态机。另外，对于在「已修改」或者「独占」状态的 Cache Line，修改更新其数据不需要发送广播给其他 CPU 核心

---

## 什么是软中断

在计算机中，中断是系统用来响应硬件设备请求的一种机制，操作系统收到硬件的中断请求，会打断正在执行的进程转去处理更高优先级的任务，然后调用内核中的中断处理程序来响应请求。

中断是一种异步的事件处理机制，可以提高系统的并发处理能力。 操作系统收到了中断请求，会打断其他进程的运行，所以中断请求的响应程序，也就是**中断处理程序**，要尽可能快的执行完，这样可以减少对正常进程运行调度地影响。 而且，中断处理程序在响应中断时，可能还会「临时关闭中断」，这意味着，如果当前中断处理程序没有执行完之前，系统中其他的中断请求都无法被响应，也就说**中断有可能会丢失**，所以中断处理程序要**短且快**。

Linux 系统为了解决中断处理程序执行过长和中断丢失的问题，将中断过程分成了两个阶段，分别是「上半部和下半部分」。 

- 上半部用来快速处理中断，一般会暂时关闭中断请求，主要负责处理跟硬件紧密相关或者时间敏感的事情。 
- 下半部用来延迟处理上半部未完成的工作，一般以「内核线程」的方式运行。

网卡收到网络包后，通过 DMA 方式将接收到的数据写入内存，接着会通过硬件中断通知内核有新的数据到了，于是内核就会调用对应的中断处理程序来处理该事件，这个事件的处理也是会分成上半部和下半部。 上部分要做的事情很少，会先禁止网卡中断，避免频繁硬中断，而降低内核的工作效率。接着，内核会触发一个**软中断**，把一些处理比较耗时且复杂的事情，交给「软中断处理程序」去做，也就是中断的下半部，其主要是需要从内存中找到网络数据，再按照网络协议栈，对网络数据进行逐层解析和处理，最后把数据送给应用程序。 所以，中断处理程序的上部分和下半部可以理解为： 

- 上半部直接处理硬件请求，也就是硬中断，主要是负责耗时短的工作，特点是快速执行； 
- 下半部是由内核触发，也就说软中断，主要是负责上半部未完成的工作，通常都是耗时比较长的事情，特点是延迟执行；

---

## 为什么0.1+0.2 不等于 0.3

**为什么负数要用补码表示？** 

使用原码存在的问题：需要特殊处理判断数字是否为负，如果是负数就要把加法变为减法才能得到正确结果

<img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost3@main/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E6%B5%AE%E7%82%B9/%E9%9D%9E%E5%8F%8D%E7%A0%81%E8%BF%90%E7%AE%97.png" alt="img" style="zoom:50%;" />

负数之所以用补码的方式来表示，主要是为了统一和正数的加减法操作一样，毕竟数字的加减法是很常用的一个操作，就不要搞特殊化，尽量以统一的方式来运算。 

<img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost3@main/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E6%B5%AE%E7%82%B9/%E8%A1%A5%E7%A0%81%E8%BF%90%E7%AE%97%E8%BF%87%E7%A8%8B.png" alt="img" style="zoom:50%;" />



**十进制小数怎么转成二进制？** 

十进制整数转二进制使用的是「除 2 取余法」，十进制小数使用的是「乘 2 取整法」。 

**计算机是怎么存小数的？** 

计算机是以浮点数的形式存储小数的，大多数计算机都是 IEEE 754 标准定义的浮点数格式，包含三个部分： 

- 符号位：表示数字是正数还是负数，为 0 表示正数，为 1 表示负数； 
- 指数位：指定了小数点在数据中的位置，指数可以是负数，也可以是正数，指数位的长度越长则数值的表达范围就越大； 
- 尾数位：小数点右侧的数字，也就是小数部分，比如二进制 1.0011 x 2^(-2)，尾数部分就是 0011，而且尾数的长度决定了这个数的精度，因此如果要表示精度更高的小数，则就要提高尾数位的长度； 

用 32 位来表示的浮点数，则称为单精度浮点数，也就是我们编程语言中的 float 变量，而用 64 位来表示的浮点数，称为双精度浮点数，也就是 double 变量。

**0.1 + 0.2 == 0.3 吗？** 

不是的，0.1 和 0.2 这两个数字用二进制表达会是一个一直循环的二进制数，比如 0.1 的二进制表示为 0.0 0011 0011 0011… （0011 无限循环)，对于计算机而言，0.1 无法精确表达，这是浮点数计算造成精度损失的根源。 因此，IEEE 754 标准定义的浮点数只能根据精度舍入，然后用「近似值」来表示该二进制，那么意味着计算机存放的小数可能不是一个真实值。 0.1 + 0.2 并不等于完整的 0.3，这主要是因为这两个小数无法用「完整」的二进制来表示，只能根据精度舍入，所以计算机里只能采用近似数的方式来保存，那两个近似数相加，得到的必然也是一个近似数。

## 为什么8位有符号数的取值范围是-128 ~127

[为什么8位有符号数的取值范围是-128 ~127-CSDN博客](https://blog.csdn.net/yanlaifan/article/details/84344979)

[8位有符号数的取值范围下限为什么是-128？ - 知乎](https://zhuanlan.zhihu.com/p/372600243)

第一位为符号位，正数为0，负数为1，负数使用补码表示（负数的补码 = 其绝对值的二进制表示按位取反 + 1。**）

正数最大值：0111 1111 即127

最小值是：1111 1111吗？，计算机存储的是补码，1111 1111的补码为 1000 0001，可以发现这个补码还能再减1，最终得到补码1000 0000表示-128；同时可以检验-128的补码为 1000 0000（绝对值的二进制）-> 0111 1111（按位取反）-> 1000 0000 (加一)

负数最小值：1000 0000 为-128（可以认为是-0）

---

## 内核

什么是内核呢？ 

计算机是由各种外部硬件设备组成的，比如内存、cpu、硬盘等，如果每个应用都要和这些硬件设备对接通信协议，那这样太累了，所以这个中间人就由内核来负责，让内核作为应用连接硬件设备的桥梁，应用程序只需关心与内核交互，不用关心硬件的细节。

<img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E5%86%85%E6%A0%B8/Kernel_Layout.png" alt="内核" style="zoom:50%;" />

内核有哪些能力呢？ 现代操作系统，内核一般会提供 4 个基本能力： 

- 管理进程、线程，决定哪个进程、线程使用 CPU，也就是进程调度的能力； 
- 管理内存，决定内存的分配和回收，也就是内存管理的能力； 
- 管理硬件设备，为进程与硬件设备之间提供通信能力，也就是硬件通信能力； 
- 提供系统调用，如果应用程序要运行更高权限运行的服务，那么就需要有系统调用，它是用户程序与操作系统之间的接口。 

内核是怎么工作的？ 

内核具有很高的权限，可以控制 cpu、内存、硬盘等硬件，而应用程序具有的权限很小，因此大多数操作系统，把内存分成了两个区域： 

- 内核空间，这个内存空间只有内核程序可以访问； 
- 用户空间，这个内存空间专门给应用程序使用； 

用户空间的代码只能访问一个局部的内存空间，而内核空间的代码可以访问所有内存空间。因此，当程序使用用户空间时，我们常说该程序在**用户态**执行，而当程序使内核空间时，程序则在**内核态**执行。 

应用程序如果需要进入内核空间，就需要通过**系统调用**，下面来看看系统调用的过程：

<img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E5%86%85%E6%A0%B8/systemcall.png" alt="img" style="zoom: 67%;" />

内核程序执行在内核态，用户程序执行在用户态。当应用程序使用系统调用时，会产生一个中断。发生中断后， CPU 会中断当前在执行的用户程序，转而跳转到中断处理程序，也就是开始执行内核程序。内核处理完后，主动触发中断，把 CPU 执行权限交回给用户程序，回到用户态继续工作。

---

### 宏内核 vs 微内核

宏内核是一种**将所有核心功能集成在一个内核空间**运行的操作系统架构。所有操作系统核心组件（如进程管理、内存管理、文件系统、设备驱动等）都运行在**内核模式**，并共享同一个地址空间。

特点

- **高性能**：所有服务运行在内核态，调用开销低。
- **紧密集成**：所有功能紧密耦合，代码量大。
- **稳定性较差**：一个模块崩溃可能导致整个系统崩溃。
- **扩展性差**：修改或增加功能需要重新编译整个内核。

代表操作系统

- **Linux**
- **Windows（混合内核，但接近宏内核）**
- **Unix（早期版本）**
- **BSD 系**

微内核是一种**将操作系统的核心功能精简到最小**，仅保留基本功能（如进程管理、内存管理、IPC），而将文件系统、设备驱动、网络协议等非核心功能移到**用户空间**，通过**消息传递（IPC）**进行通信。

特点

- **高安全性**：由于驱动、文件系统等运行在用户空间，内核崩溃不会影响整个系统。
- **可扩展性强**：可以动态加载或更换组件，无需重启内核。
- **性能较低**：用户态与内核态之间的**频繁通信**增加了开销。
- **实现复杂**：设计 IPC 机制和模块间通信较困难。

代表操作系统

- 鸿蒙
- **macOS（XNU 混合内核，部分微内核特性）**

混合类型内核，它的架构有点像微内核，内核里面会有一个最小版本的内核，然后其他模块会在这个基础上搭建，然后实现的时候会跟宏内核类似，也就是把整个内核做成一个完整的程序，大部分服务都在内核中，这就像是宏内核的方式包裹着一个微内核

![分别为宏内核、微内核、混合内核的操作系统结构](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E5%86%85%E6%A0%B8/OS-structure2.png)