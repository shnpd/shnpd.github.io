---
title: 对BP神经网络的理解及5.13公式推导
toc: true
date: 2021-07-22 10:42:27
tags: 机器学习 神经网络
categories: 机器学习
---

​​点击阅读更多查看文章内容<!--more-->

# BP神经网络

## 概述
误差逆传播(errorBackPropagation,简称BP)算法是迄今最成功的神经网络学习算法.现实任务中使用神经网络时,大多是在使用BP算法进行训练.值得指出的是,BP算法不仅可用于多层前馈神经网络,还可用于其他类型的神经网络,例如训练递归神经网络[Pineda,1987].但通常说“BP网络”时，一般是指用BP算法训练的多层前馈神经网络.

## 算法
**一、简要思想**
BP算法的全称是误差逆传播算法，顾名思义。BP算法就是根据当前数据产生的误差进行逆向传播修改参数，从而不断获得更合适的参数的过程。整个算法分为数据前传获得结果以及误差逆传修改参数两部分。
**二、参数解释**
给定训练集D = {(x1, y1),(x2, y2),.. . , (xm,ym)},xi∈ Rd, yi∈Rl,即输入示例由d个属性描述,输出l维实值向量.
下图给出了一个拥有d个输入神经元、l个输出神经元、q个隐层神经元的多层前馈网络结构,
θj——输出层第j个神经元的阈值
γh——隐层第h个神经元的阈值
vih——输入层第i个神经元与隐层第h个神经元之间的连接权
whj——隐层第h个神经元与输出层第j个神经元之间的连接权
αh——隐层第h个神经元接收到的输入
βj——输出层第j个神经元接收到的输入
bh——隐层第h个神经元的输出![在这里插入图片描述](https://cdn.jsdelivr.net/gh/shnpd/blog-pic@main/csdn/347da7cb4e41a9883317024efd02f139_1740931359319.png)
**三、误差逆传播过程**
1.根据训练例求出对应的均方误差Ek
2.基于梯度下降策略根据Ek以及给定的学习率η求出对应的Δwhj、Δθj、Δvih、Δγh
3.更新whj、θj、vih、γh

## 累积BP
我们上面介绍的“标准BP算法”每次仅针对一个训练样例更新连接权和阈值，也就是说，算法的更新规则是基于单个的Ek推导而得.如果类似地推导出基于累积误差最小化的更新规则,就得到了累积误差逆传播(accumulated error backpropagation)算法.累积BP算法与标准BP算法都很常用.一般来说,标准BP算法每次更新只针对单个样例,参数更新得非常频繁,而且对不同样例进行更新的效果可能出现“抵消”现象.因此,为了达到同样的累积误差极小点,标准BP算法往往需进行更多次数的迭代.累积BP算法直接针对累积误差最小化,它在读取整个训练集D一遍后才对参数进行更新，其参数更新的频率低得多.但在很多任务中，累积误差下降到一定程度之后,进一步下降会非常缓慢,这时标准BP往往会更快获得较好的解，尤其是在训练集D非常大时更明显.

## 过拟合
由于其强大的表示能力，BP神经网络经常遭遇过拟合,其训练误差持续降低,但测试误差却可能上升.有两种策略常用来缓解BP网络的过拟合.
第一种策略是“早停”(early stopping):将数据分成训练集和验证集,训练集用来计算梯度、更新连接权和阈值,验证集用来估计误差,若训练集误差降低但验证集误差升高,则停止训练,同时返回具有最小验证集误差的连接权和阈值.
第二种策略是“正则化”(regularization) 其基本思想是在误差目标函数中增加一个用于描述网络复杂度的部分，例如连接权与阈值的平方和.仍令Ek表示第k个训练样例上的误差，w表示连接权和阈值,则误差目标函数改变为
![在这里插入图片描述](https://cdn.jsdelivr.net/gh/shnpd/blog-pic@main/csdn/de502c727e6eddb9ea9bed79672219ff_1740931359319.png)
其中λ∈ (0,1)用于对经验误差与网络复杂度这两项进行折中,常通过交叉验证法来估计.

## 5.13公式推导
![在这里插入图片描述](https://cdn.jsdelivr.net/gh/shnpd/blog-pic@main/csdn/1bf0016419895e043b7cdeeac8a029c3_1740931359319.png)
![在这里插入图片描述](https://cdn.jsdelivr.net/gh/shnpd/blog-pic@main/csdn/92269ef29b38458e3ae8d2941c555011_1740931367206.jpeg)

